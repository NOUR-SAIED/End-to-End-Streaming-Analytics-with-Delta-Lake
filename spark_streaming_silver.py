#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script Silver Layer uniquement : Bronze -> Delta Silver
"""

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
import time

print("ğŸš€ Initialisation de Spark - Silver Layer...")

# Initialiser Spark avec support Delta
spark = SparkSession.builder \
    .appName("SilverLayer-BronzeAggregation") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.memory", "2g") \
    .config("spark.sql.streaming.checkpointLocation", "/tmp/delta/checkpoints") \
    .config("spark.sql.shuffle.partitions", "2") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")
print("âœ… Spark Session crÃ©Ã©e avec succÃ¨s")

print("\n" + "=" * 80)
print("FLUX SILVER : Bronze â†’ Delta Silver (AgrÃ©gations)")
print("=" * 80)

# VÃ©rifier que la table Bronze existe
bronze_path = "/tmp/delta/bronze/ventes_stream"
print(f"ğŸ” VÃ©rification de la table Bronze : {bronze_path}")

max_retries = 10
retry_count = 0

while retry_count < max_retries:
    try:
        test_df = spark.read.format("delta").load(bronze_path)
        bronze_count = test_df.count()
        print(f"âœ… Table Bronze trouvÃ©e avec {bronze_count} enregistrements")
        break
    except Exception as e:
        retry_count += 1
        print(f"â³ Tentative {retry_count}/{max_retries} - Table Bronze pas encore prÃªte...")
        if retry_count == max_retries:
            print("âŒ Erreur : La table Bronze n'existe pas encore.")
            print("   Veuillez d'abord dÃ©marrer le flux Bronze (spark_streaming_bronze.py)")
            spark.stop()
            exit(1)
        time.sleep(5)

# Lecture du flux Bronze
print("\nğŸ“– Lecture du flux Bronze en streaming...")
df_silver_stream = spark.readStream \
    .format("delta") \
    .load(bronze_path)

print("âœ… Lecture du flux Bronze Ã©tablie")

# Application du Watermark pour gÃ©rer les donnÃ©es tardives
df_silver_watermarked = df_silver_stream.withWatermark("timestamp_event", "10 minutes")

print("âœ… Watermark appliquÃ© (10 minutes)")

# AgrÃ©gation des donnÃ©es par fenÃªtre de temps et dimensions
df_silver_agg = df_silver_watermarked.groupBy(
    F.window(F.col("timestamp_event"), "1 minute", "1 minute").alias("window"),
    "client_id",
    "client_nom",
    "pays",
    "segment"
).agg(
    F.sum("quantite").alias("total_quantite"),
    F.sum("montant").alias("total_depense"),
    F.count("*").alias("nb_achats"),
    F.avg("montant").alias("panier_moyen")
).withColumn("est_client_fidele", F.when(F.col("nb_achats") >= 2, True).otherwise(False)) \
 .withColumn("window_start", F.col("window.start")) \
 .withColumn("window_end", F.col("window.end")) \
 .drop("window")

print("âœ… AgrÃ©gations configurÃ©es (fenÃªtres de 1 minute)")

# Ã‰criture en Delta Lake Silver
query_silver = df_silver_agg.writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/tmp/delta/checkpoints/ventes_silver") \
    .start("/tmp/delta/silver/ventes_aggreges")

print("âœ… Streaming Silver dÃ©marrÃ©")

print("\n" + "=" * 80)
print("ğŸ“Š SILVER LAYER ACTIVE")
print("=" * 80)
print("Silver Layer : /tmp/delta/silver/ventes_aggreges")
print("\nâ³ Flux actif. Appuyez sur Ctrl+C pour arrÃªter...")
print("=" * 80)

# Attendre la fin du flux
try:
    query_silver.awaitTermination()
except KeyboardInterrupt:
    print("\nâ¹ï¸  ArrÃªt du flux Silver...")
    query_silver.stop()
    print("âœ… Flux Silver arrÃªtÃ© proprement")
finally:
    spark.stop()
    print("âœ… Spark Session fermÃ©e")